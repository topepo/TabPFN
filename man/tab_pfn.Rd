% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TabPFN-fit.R
\name{tab_pfn}
\alias{tab_pfn}
\alias{tab_pfn.default}
\alias{tab_pfn.data.frame}
\alias{tab_pfn.matrix}
\alias{tab_pfn.formula}
\alias{tab_pfn.recipe}
\title{Fit a TabPFN model.}
\usage{
tab_pfn(x, ...)

\method{tab_pfn}{default}(x, ...)

\method{tab_pfn}{data.frame}(
  x,
  y,
  num_estimators = 8L,
  softmax_temperature = 0.9,
  balance_probabilities = FALSE,
  average_before_softmax = FALSE,
  training_set_limit = 10000,
  control = control_tab_pfn(),
  ...
)

\method{tab_pfn}{matrix}(
  x,
  y,
  num_estimators = 8L,
  softmax_temperature = 0.9,
  balance_probabilities = FALSE,
  average_before_softmax = FALSE,
  training_set_limit = 10000,
  control = control_tab_pfn(),
  ...
)

\method{tab_pfn}{formula}(
  formula,
  data,
  num_estimators = 8L,
  softmax_temperature = 0.9,
  balance_probabilities = FALSE,
  average_before_softmax = FALSE,
  training_set_limit = 10000,
  control = control_tab_pfn(),
  ...
)

\method{tab_pfn}{recipe}(
  x,
  data,
  num_estimators = 8L,
  softmax_temperature = 0.9,
  balance_probabilities = FALSE,
  average_before_softmax = FALSE,
  training_set_limit = 10000,
  control = control_tab_pfn(),
  ...
)
}
\arguments{
\item{x}{Depending on the context:
\itemize{
\item A \strong{data frame} of predictors.
\item A \strong{matrix} of predictors.
\item A \strong{recipe} specifying a set of preprocessing steps
created from \code{\link[recipes:recipe]{recipes::recipe()}}.
}}

\item{...}{Not currently used, but required for extensibility.}

\item{y}{When \code{x} is a \strong{data frame} or \strong{matrix}, \code{y} is the outcome
specified as:
\itemize{
\item A \strong{data frame} with 1 numeric column.
\item A \strong{matrix} with 1 numeric column.
\item A numeric \strong{vector} for regression or a \strong{factor} for classification.
}}

\item{num_estimators}{An integer for the ensemble size. Default is \code{8L}.}

\item{softmax_temperature}{An adjustment factor that is a divisor in the
exponents of the softmax function (see Details below). Defaults to 0.9.}

\item{balance_probabilities}{A logical to adjust the prior probabilities in
cases where there is a class imbalance. Default is \code{FALSE}. Classification
only.}

\item{average_before_softmax}{A logical. For cases where
\code{num_estimators > 1}, should the average be done before using the softmax
function or after? Default is \code{FALSE}.}

\item{training_set_limit}{An integer greater than 2L (and possibly \code{Inf})
that can be used to keep the training data within the limits of the
data constraints imposed by the Python library.}

\item{control}{A list of options produced by \code{\link[=control_tab_pfn]{control_tab_pfn()}}.}

\item{formula}{A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.}

\item{data}{When a \strong{recipe} or \strong{formula} is used, \code{data} is specified as:
\itemize{
\item A \strong{data frame} containing both the predictors and the outcome.
}}
}
\value{
A \code{tab_pfn} object with elements:
\itemize{
\item \code{fit}: the python object containing the model.
\item \code{levels}: a character string of class levels (or NULL for regression)
\item \code{training}: a vector with the training set dimensions.
\item \code{logging}: any R or python messages produced by the computations.
\item \code{blueprint}: am object produced by \code{\link[hardhat:mold]{hardhat::mold()}} used to process
new data during prediction.
}
}
\description{
\code{tab_pfn()} applies data to a pre-estimated deep learning model defined by
Hollmann \emph{et al} (2025). This model emulates Bayesian inference for
regression and classification models.
}
\details{
\subsection{Computing Requirements}{

This model can be used with or without a graphics processing unit (GPU).
However, it is fairly limited when used with a CPU (and no GPU). There might
be additional data size limitation warnings with CPU computations, and,
understandably, the execution time is much longer. CPU computations can also
consume a significant amount of system memory, depending on the size of your
data.

GPUs using CUDA (Compute Unified Device Architecture) are most effective.
Limited testing with others has shown that GPUs with Metal Performance
Shaders (MPS) instructions (e.g., Apple GPUs) have limited utility for these
specific computations and might be slower than the CPU for some data sets.
}

\subsection{License Requirements}{

On November 6, 2025, PriorLabs released version 2.5 of the model, which
contained several improvements. One other change is that accessing the model
parameters required an API key. Without one, an error occurs:

"This model is gated and requires you to accept its terms.  Please
follow these steps: 1. Visit https://huggingface.co/Prior-Labs/tabpfn_2_5
in your browser and accept the terms of use. 2. Log in to your Hugging Face
account via the command line by running: hf auth login (Alternatively, you
can set the HF_TOKEN environment variable with a read token)."

The license contains provisions for "Non-Commercial Use Only" usage if that
is relevant for you.

To get an API key, use the \code{huggingface} link above, create an account, and
then get an API key. Once you have that, put it in your \code{.Renviron} file in
the form of:

\preformatted{
HF_TOKEN=your_api_key_value
}

The \pkg{usethis} function \code{edit_r_environ()} can be very helpful here.
}

\subsection{Python Installation}{

You will need a working Python virtual environment with the correct packages
to use these modeling functions.

There are at least two ways to proceed.

The first approach, which we \emph{strongly suggest}, is to simply load this
package and attempt to run a model. This will prompt \pkg{reticulate} to
create an ephemeral environment and automatically install the required
packages. That process would look like this:

\preformatted{
  > library(TabPFN)
  >
  > predictors <- mtcars[, -1]
  > outcome <- mtcars[, 1]
  >
  > # XY interface
  > mod <- tab_pfn(predictors, outcome)
  Downloading uv...Done!
  Downloading cpython-3.12.12 (download) (15.9MiB)
   Downloading cpython-3.12.12 (download)
  Downloading setuptools (1.1MiB)
  Downloading scikit-learn (8.2MiB)
  Downloading numpy (4.9MiB)

  <downloading and installing more packages>

   Downloading llvmlite
   Downloading torch
  Installed 58 packages in 350ms
  > mod
  TabPFN Regression Model

  Training set
  ℹ 32 data points
  ℹ 10 predictors
}

The location of the environment can be found at
\code{tools::R_user_dir("reticulate", "cache")}.

Alternatively, you can use the functions in the \pkg{reticulate} package to
create a virtual environment and install the required Python packages there.
An example pattern is:

\preformatted{
  library(reticulate)
  env_name <- "r-tabpfn"      # exact name can be different
  py_version <- "3.13:latest" # or your version of choice

  virtualenv_list()
  py_inst <- install_python(py_version)

  # Save py_inst value
  <restart R>

  library(reticulate)
  use_python_version(py_version)
  virtualenv_create(env_name, python = py_inst)
}

Once you have that virtual environment installed, you can load it before
starting \pkg{TabPFN}:

\preformatted{
  reticulate::use_virtualenv("r-tabpfn")
}

Please see the important note below.

\strong{Important Note}. Due to how Python uses the OpenMP library, it is
important that you load your virtual Python environment prior to loading any
R package that also uses OpenMP. If not, a segmentation fault can occur.
See \href{https://github.com/topepo/TabPFN/issues/3}{this GitHub issue}.
}

\subsection{Data}{

Be default, there are limits to the training data dimensions:
\itemize{
\item Version 2.0: number of training set samples (10,000) and, the number of
predictors (500). There is an unchangeable limit to the number of classes
(10).
\item Version 2.5: number of training set samples (50,000) and, the number of
predictors (2,000). There is an unchangeable limit to the number of classes
(10).
}

Predictors do not require preprocessing; missing values and factor vectors
are allowed.
}

\subsection{Calculations}{

For the \code{softmax_temperature} value, the softmax terms are:

\preformatted{
exp(value / softmax_temperature)
}

A value of \code{softmax_temperature = 1} results in a plain softmax value.
}
}
\examples{
predictors <- mtcars[, -1]
outcome <- mtcars[, 1]

# XY interface
mod <- tab_pfn(predictors, outcome)

# Formula interface
mod2 <- tab_pfn(mpg ~ ., mtcars)

# Recipes interface
if (rlang::is_installed("recipes")) {
 library(recipes)
 rec <-
  recipe(mpg ~ ., mtcars) \%>\%
  step_log(disp)

 mod3 <- tab_pfn(rec, mtcars)
 mod3
}

}
\references{
Hollmann, Noah, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max
Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter.
"Accurate predictions on small data with a tabular foundation model."
\emph{Nature} 637, no. 8045 (2025): 319-326.

Hollmann, Noah, Samuel Müller, Katharina Eggensperger, and Frank Hutter.
"Tabpfn: A transformer that solves small tabular classification problems in
a second." \emph{arXiv preprint} arXiv:2207.01848 (2022).

Müller, Samuel, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and
Frank Hutter. "Transformers can do Bayesian inference." \emph{arXiv preprint}
arXiv:2112.10510 (2021).
}
